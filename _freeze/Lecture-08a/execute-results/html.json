{
  "hash": "759548051c1ef5f320f1fdbf6c188ec8",
  "result": {
    "markdown": "---\ntitle: \"Regression model development\"\nsubtitle: \"ENVX2001 - Applied Statistical Methods\"\nauthor:\n  - name: Liana Pozza\n    affiliations: The University of Sydney\ndate: last-modified\nself-contained: true\nexecute:\n  freeze: auto\n  cache: false\n# NOTE: please check _quarto.yml file for more options\n---\n\n\n\n\n\n# Variable selection\n\n> \"The hardest thing to learn in life is which bridge to cross and which to burn.\"\n\n-- [David Russell](https://en.wikipedia.org/wiki/David_Russell_(guitarist))\n\n\n## Workflow {auto-animate=true}\n\n1. Model development\n    + Explore: visualise, summarise\n    + Transform predictors: linearise, reduce skewness/leverage\n    + Model: fit, check assumptions, interpret, transform. Repeat.\n  \n2. Variable selection\n    + VIF: remove predictors with high variance inflation factor\n    + Model selection: stepwise selection, AIC, principle of parsimony, assumption checks\n  \n3. Predictive modelling\n    + Predict: Use the model to predict new data\n    + Validate: Evaluate the model’s performance\n\n\n\n## Workflow {auto-animate=true}\n\n\n\n\n\n  \n2. Variable selection\n    + VIF: remove predictors with high variance inflation factor\n    + Model selection: stepwise selection, AIC, principle of parsimony, assumption checks\n  \n \n \n\n## Previously on ENVX2001... {auto-animate=\"true\"}\n\n\nWe fitted a multiple linear regression model to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmulti_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(multi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n## Question {auto-animate=\"true\"}\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n. . .\n\n**Are all the variables/predictors needed?**\n\n. . .\n\n \n### Principles\n\nA good model:\n\n- Has only *useful* predictors: principle of parsimony\n- Has *no redundant* predictors: principle of orthogonality (no multicollinearity)\n- Is *interpretable* (principle of transparency; last week), or *predicts* well (principle of accuracy; next week)\n\n## On the principle of parsimony\n\n- [Ockham's razor](https://en.wikipedia.org/wiki/Occam%27s_razor): \"Entities should not be multiplied unnecessarily.\"\n  - One should prefer the *simplest* explanation that fits the data if multiple explanations are equally good.\n\n> \"It is vain to do with more what can be done with fewer.\"\n\n-- [William of Ockham](https://en.wikipedia.org/wiki/William_of_Ockham) (1287--1347)\n\n## Variance-bias trade-off\n\n- The more predictors we include, the more variance we can explain.\n- However, the more predictors we include, *the more bias we introduce*.\n\n### What happens when we add more predictors to a model?\n\nA simple example using polynomial regression.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nset.seed(1030)\nxsquared <- function(x) {\n  x ^ 2\n}\n# Generate xy data\nsim_data <- function(xsquared, sample_size = 100) {\n  x = runif(n = sample_size, min = 0, max = 1) \n  y = rnorm(n = sample_size, mean = xsquared(x), sd = 0.05)\n  data.frame(x, y)\n}\n# Generate predicted data (model)\ndf = sim_data(xsquared, sample_size = 60)\nfit_1 <- lm(y ~ 1, data = df)\nfit_2 <- lm(y ~ poly(x, degree = 1), data = df)\nfit_3 <- lm(y ~ poly(x, degree = 2), data = df)\nfit_many <- lm(y ~ poly(x, degree = 20), data = df)\ntruth <- seq(from = 0, to = 1, by = 0.01)\n# Combine the data and model fits into a single data frame\ndf <- data.frame(\n  x = df$x,\n  y = df$y,\n  fit_1 = predict(fit_1),\n  fit_2 = predict(fit_2),\n  fit_3 = predict(fit_3),\n  fit_many = predict(fit_many)\n)\n```\n:::\n\n\n\n## Variance-bias trade-off\n\n- The more predictors we include, the more variance we can explain.\n- However, the more predictors we include, *the more bias we introduce*.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Reshape the data frame into long format\ndf_long <- pivot_longer(\n  df, \n  cols = starts_with(\"fit_\"),\n  names_to = \"model\",\n  values_to = \"value\"\n) %>% \n  mutate(\n    model = case_when(\n      model == \"fit_1\" ~ \"y = b\",\n      model == \"fit_2\" ~ \"y = b + mx\",\n      model == \"fit_3\" ~ \"y = b + mx + nx^2\",\n      model == \"fit_many\" ~ \"y = b + mx + nx^2 + ... + zx^20\",\n      TRUE ~ model\n    )\n  )\n# Plot\np <- ggplot(df_long, aes(x = x, y = value, color = model)) +\n  facet_wrap(~ model, ncol = 2, scales = \"free\") +\n  geom_point(aes(y = y), alpha = .4, size = 2) +\n  geom_line(linewidth = .9, linetype = 1) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\") +\n  geom_blank()\np\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n---\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n- As complexity increases, bias *decreases* (the mean of a model’s predictions is closer to the true mean). However, it may (sometimes) increase again if the model is too complex.\n- As complexity increases, variance (the variance about the mean of a model’s predictions) *may first* decrease, but it *increases* as the predictions become more *wild*.\n- The **goal** is to find a model that isn't too complex, and also has a \"balance\" between **bias and variance**.\n\n## Bias-variance plot\n\n![](assets/bias_var.svg)\n\n## How do we determine the best model?\n\n- **Partial F-test**: compare the full model to a reduced model, works well when the number of predictors is small and models are *nested* (more on this later).\n- **Stepwise regression**: add/remove predictors one at a time, works well when the number of predictors is large and the main aim is to **interpret** the model.\n- **Cross-validation**: leave-one-out, k-fold, etc., works well when the number of predictors is large and main aim is to **predict** the model. *Note: can be used with stepwise regression.*\n\n# Partial F-test\n\n## Air quality: can we reduce the number of predictors? {auto-animate=\"true\"}\n\n**Full model:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(multi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n- 3 predictors: `Temp`, `Solar.R`, `Wind`\n- `Wind` has the highest p-value, can we remove it?\n- Full model: multiple R-squared = 0.66, adjusted R-squared = 0.66\n\n## {auto-animate=\"true\"}\n\n- `Wind` has the highest p-value, can we remove it?\n- Full model: multiple R-squared = 0.66, adjusted R-squared = 0.66\n\n. . .\n\n**Reduced model: take out `Wind`**\n\n::: {.cell}\n\n```{.r .cell-code}\nreduced_fit <- lm(log(Ozone) ~ Temp + Solar.R, data = airquality)\nsummary(reduced_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83864 -0.33727  0.03444  0.29877  1.38210 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.7646527  0.4249016  -4.153 6.58e-05 ***\nTemp         0.0607386  0.0056663  10.719  < 2e-16 ***\nSolar.R      0.0024651  0.0005924   4.161 6.38e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5413 on 108 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6163,\tAdjusted R-squared:  0.6092 \nF-statistic: 86.73 on 2 and 108 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n. . .\n\n- Reduced model: multiple R-squared = 0.62, adjusted R-squared = 0.61\n- **Adjusted R-squared is lower, but is a 0.04 difference \"worth it\"?**\n\n(As the full model has a higher adjusted R-squared, we can already conclude that the full model is better than the reduced model, but let's answer the question anyway.)\n\n## The r^2^ value\n\nThe R-squared value is the proportion of variance explained by the model.\n\n$$ r^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n\nThe adjusted R-squared value is the proportion of variance explained by the model, adjusted for the number of predictors.\n\n$$ r^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} $$\n\nwhere $n$ is the number of observations and $p$ is the number of predictors.\n\n:::{.callout-tip}\nThe $r^2$ value does not change when we add or remove predictors (i.e. contains *no* information about model complexity), but the adjusted $r^2$ value does.\n:::\n\n## Partial F-test {auto-animate=\"true\"}\n\nHow much of an improvement in adjusted $r^2$ is worth having an extra variable / more complex model?\n\n- We can perform a hypothesis test to determine whether the improvement is significant.\n- The **partial F-test** compares the full model to a reduced model in terms of the trade-off between model complexity and variance explained (i.e. **adjusted $r^2$**).\n  - $H_0$: no significant difference between the full and reduced models\n  - $H_1$: the full model is significantly better than the reduced model\n  - Calculating the F-stat:\n\n$$F = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}$$\n\n## Partial F-test: calculation {auto-animate=\"true\"}\n\n$$F = \\big| \\frac{SS_{reg,full} - SS_{reg,reduced}}{(df_{res,full} - df_{res,reduced})} \\big | \\div MS_{res, full}$$\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nfull <- anova(multi_fit) %>% broom::tidy()\nfull\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  term         df sumsq meansq statistic   p.value\n  <chr>     <int> <dbl>  <dbl>     <dbl>     <dbl>\n1 Temp          1 45.8  45.8       177.   2.07e-24\n2 Solar.R       1  5.07  5.07       19.6  2.29e- 5\n3 Wind          1  3.97  3.97       15.4  1.58e- 4\n4 Residuals   107 27.7   0.259      NA   NA       \n```\n:::\n\n```{.r .cell-code}\nreduced <- anova(reduced_fit) %>% broom::tidy()\nreduced\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 6\n  term         df sumsq meansq statistic   p.value\n  <chr>     <int> <dbl>  <dbl>     <dbl>     <dbl>\n1 Temp          1 45.8  45.8       156.   1.05e-22\n2 Solar.R       1  5.07  5.07       17.3  6.38e- 5\n3 Residuals   108 31.6   0.293      NA   NA       \n```\n:::\n:::\n\n:::\n::: {.column width=\"50%\"}\n- $SS_{reg,full} = 45.8 + 5.07 + 3.97 = 54.84$\n- $SS_{reg,reduced} = 45.8 + 5.07 = 50.87$\n- $df_{res,full} = 107$\n- $df_{res,reduced} = 108$\n\n$F = |\\frac{54.84 - 50.87}{(107-108)}| \\div 0.259 = 15.35$\n\n:::\n::::\n\n## \n\nIn R (manually):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregss_full <- sum(full$sumsq[1:3])\nregss_reduced <- sum(reduced$sumsq[1:2])\nresdf_full <- full$df[4]\nresdf_reduced <- reduced$df[3]\nresms_full <- full$meansq[4]\nF <- abs((regss_full - regss_reduced) / ((resdf_full - resdf_reduced))) / resms_full\nF\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.35026\n```\n:::\n\n```{.r .cell-code}\npf(F, df1 = 1, df2 = resdf_full, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0001576806\n```\n:::\n:::\n\n\n. . .\n\nAlternatively (using functions):\n\n::: {.cell}\n\n```{.r .cell-code}\npf_result <- anova(multi_fit, reduced_fit)\npf_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: log(Ozone) ~ Temp + Solar.R + Wind\nModel 2: log(Ozone) ~ Temp + Solar.R\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1    107 27.675                                 \n2    108 31.645 -1   -3.9703 15.35 0.0001577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n- The partial F-test is significant (p-value < 0.05), so we can reject the null hypothesis and conclude that the full model is significantly better (as expected)\n\n\n## But wait... {auto-animate=\"true\"}\n\n. . .\n\nLooking back at the original model, we can see that the partial regression coefficients are the *same* as the partial F-test results!\n\n\n::: {.cell}\n\n```{.r .cell-code}\npf_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: log(Ozone) ~ Temp + Solar.R + Wind\nModel 2: log(Ozone) ~ Temp + Solar.R\n  Res.Df    RSS Df Sum of Sq     F    Pr(>F)    \n1    107 27.675                                 \n2    108 31.645 -1   -3.9703 15.35 0.0001577 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nfull\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  term         df sumsq meansq statistic   p.value\n  <chr>     <int> <dbl>  <dbl>     <dbl>     <dbl>\n1 Temp          1 45.8  45.8       177.   2.07e-24\n2 Solar.R       1  5.07  5.07       19.6  2.29e- 5\n3 Wind          1  3.97  3.97       15.4  1.58e- 4\n4 Residuals   107 27.7   0.259      NA   NA       \n```\n:::\n:::\n\n\nThis is because the reduced model is *nested* within the full model so the partial F-test is equivalent to a partial regression coefficient test.\n\n## Nested models\n\n- Previous example is a simple example of a **nested model**.\n- A model is nested within another model if the predictors in the first model are a subset of the predictors in the second model.\n- This makes comparing the two models easier, as we can compare the regression coefficients of the two models.\n\n. . .\n\n### Example\n\n- If the original model is y ~ a + b + c + d:\n  - Nested: y ~ a + b + c\n  - Nested: y ~ a + b\n  - *Not* nested: y ~ a + b + **e** -- because **e** is not in the full model\n\n\n:::{.callout-important}\n**Partial F-tests will *only* make sense/work for nested models!**\n::: \n\n# Another example: Bird abundance\n\n## About\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn <-read_csv(\"assets/loyn.csv\") \n```\n:::\n\n\n- Can we predict the abundance of birds in forest patches cleared for agriculture, based on patch size, area, grazing and other variables?\n- Loyn ([1987](https://www.researchgate.net/profile/Richard-Loyn/publication/279541149_Effects_of_patch_area_and_habitat_on_bird_abundances_species_numbers_and_tree_health_in_fragmented_Victoria_forests/links/563ae1bc08ae337ef2985592/Effects-of-patch-area-and-habitat-on-bird-abundances-species-numbers-and-tree-health-in-fragmented-Victoria-forests.pdf))\n  - DIST: Distance to nearest patch (km)\n  - LDIST: Distance to a larger patch (km)\n  - AREA: Patch area (ha)\n  - GRAZE: Grazing pressure 1(light) – 5 (heavy) – ALT: Altitude (m)\n  - YR.ISOL: Years since isolation (years)\n\n## Full model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 56\nColumns: 7\n$ ABUND   <dbl> 5.3, 2.0, 1.5, 17.1, 13.8, 14.1, 3.8, 2.2, 3.3, 3.0, 27.6, 1.8…\n$ AREA    <dbl> 0.1, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.…\n$ YR.ISOL <dbl> 1968, 1920, 1900, 1966, 1918, 1965, 1955, 1920, 1965, 1900, 19…\n$ DIST    <dbl> 39, 234, 104, 66, 246, 234, 467, 284, 156, 311, 66, 93, 39, 40…\n$ LDIST   <dbl> 39, 234, 311, 66, 246, 285, 467, 1829, 156, 571, 332, 93, 39, …\n$ GRAZE   <dbl> 2, 5, 5, 3, 5, 3, 5, 5, 4, 5, 3, 5, 2, 1, 5, 5, 3, 3, 3, 2, 2,…\n$ ALT     <dbl> 160, 60, 140, 160, 140, 130, 90, 60, 130, 130, 210, 160, 210, …\n```\n:::\n\n```{.r .cell-code}\nloyn_fit <- lm(ABUND ~ ., data = loyn)\nsummary(loyn_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ ., data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6638  -4.6409  -0.0883   4.2858  20.1042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.097e+02  1.133e+02  -0.968  0.33791   \nAREA         8.866e-04  4.657e-03   0.190  0.84980   \nYR.ISOL      6.693e-02  5.684e-02   1.177  0.24472   \nDIST         3.811e-03  5.418e-03   0.703  0.48514   \nLDIST        1.418e-03  1.310e-03   1.082  0.28451   \nGRAZE       -3.447e+00  1.107e+00  -3.114  0.00308 **\nALT          4.772e-02  3.089e-02   1.545  0.12878   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.947 on 49 degrees of freedom\nMultiple R-squared:  0.5118,\tAdjusted R-squared:  0.452 \nF-statistic: 8.561 on 6 and 49 DF,  p-value: 2.24e-06\n```\n:::\n:::\n\n\n## Wait! Before we go on...\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     ABUND            AREA            YR.ISOL          DIST       \n Min.   : 1.50   Min.   :   0.10   Min.   :1890   Min.   :  26.0  \n 1st Qu.:12.40   1st Qu.:   2.00   1st Qu.:1928   1st Qu.:  93.0  \n Median :21.05   Median :   7.50   Median :1962   Median : 234.0  \n Mean   :19.51   Mean   :  69.27   Mean   :1950   Mean   : 240.4  \n 3rd Qu.:28.30   3rd Qu.:  29.75   3rd Qu.:1966   3rd Qu.: 333.2  \n Max.   :39.60   Max.   :1771.00   Max.   :1976   Max.   :1427.0  \n     LDIST            GRAZE            ALT       \n Min.   :  26.0   Min.   :1.000   Min.   : 60.0  \n 1st Qu.: 158.2   1st Qu.:2.000   1st Qu.:120.0  \n Median : 338.5   Median :3.000   Median :140.0  \n Mean   : 733.3   Mean   :2.982   Mean   :146.2  \n 3rd Qu.: 913.8   3rd Qu.:4.000   3rd Qu.:182.5  \n Max.   :4426.0   Max.   :5.000   Max.   :260.0  \n```\n:::\n:::\n\n\n- The predictors are on very different scales, which can cause problems for the model.\n- Transforming the predictors can help.\n\n. . .\n\n:::{.callout-tip}\nTransforming predictors can help with model fitting but it is also optional -- you can still fit a model without transforming predictors. An advantage of transforming predictors is that it may help with leverage and outlier issues.\n:::\n\n## Before transformation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloyn %>%\n  pivot_longer(-ABUND) %>%\n  ggplot(aes(value)) +\n  geom_histogram() +\n  facet_wrap(~name, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n- We will perform log~10~ transforms of `AREA`, `LDIST`, and `DIST`.\n\n## Log~10~ transformation\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# perform transformations\nloyn <- loyn %>%\n  mutate(AREA_L10 = log10(AREA),\n         LDIST_L10 = log10(LDIST),\n         DIST_L10 = log10(DIST))\n\n# View distributions again\nloyn %>%\n  select(-ALT, -GRAZE, -YR.ISOL) %>%\n  pivot_longer(-ABUND) %>%\n  ggplot(aes(value)) +\n  geom_histogram() +\n  facet_wrap(~name, scales = \"free\", ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## Effect of transformation on pairwise relationships\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# generate xy scatterplot of AREA, DIST, and LDIST, plust their log10 transformations, against ABUND\nloyn %>%\n  select(-ALT, -GRAZE, -YR.ISOL) %>%\n  pivot_longer(-ABUND) %>%\n  ggplot(aes(value, ABUND)) +\n  geom_point(linesize = 1) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~name, scales = \"free\", ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n## Transformed model\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nloyn_logfit <- lm(ABUND ~ . - AREA - LDIST - DIST, data = loyn)\nsummary(loyn_logfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ . - AREA - LDIST - DIST, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6506  -2.9390   0.5289   2.5353  15.2842 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -125.69725   91.69228  -1.371   0.1767    \nYR.ISOL        0.07387    0.04520   1.634   0.1086    \nGRAZE         -1.66774    0.92993  -1.793   0.0791 .  \nALT            0.01951    0.02396   0.814   0.4195    \nAREA_L10       7.47023    1.46489   5.099 5.49e-06 ***\nLDIST_L10     -0.64842    2.12270  -0.305   0.7613    \nDIST_L10      -0.90696    2.67572  -0.339   0.7361    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.384 on 49 degrees of freedom\nMultiple R-squared:  0.6849,\tAdjusted R-squared:  0.6464 \nF-statistic: 17.75 on 6 and 49 DF,  p-value: 8.443e-11\n```\n:::\n:::\n\n\n## Checking assumptions\n\nBefore:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(loyn_fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n## Checking assumptions\n\nAfter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(loyn_logfit)\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n# Other (important) checks\nOnce we enter the realm of multivariate statistics, there are a number of other checks that we should perform to ensure that our model is appropriate for our data.\n\n## Leverage\n\n- The leverage plot shows the influence of each observation (i.e. point) on the model.\n- Points with high leverage can have a large effect on the model when removed.\n- Identified by the Cook's distance statistic -- named after the American statistician R. Dennis Cook, who introduced the concept in 1977.\n\n:::{.callout-tip}\nThe leverage plot is a useful tool for identifying outliers and influential points, but can also be used to check for other issues such as heteroskedasticity (equal variances) and non-linearity!\n:::\n\n## Reading the leverage plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(loyn_logfit, which = c(4,5))\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n-Visually, points with Cook's distance > 0.5 are considered influential by default, but this is a somewhat arbitrary threshold.\n- In practice, you should use a threshold that is appropriate for your data and model.\n\n## Outlier detection using `performance`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(loyn_logfit, check = c(\"outliers\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_outliers(loyn_logfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.948).\n- For variable: (Whole model)\n```\n:::\n:::\n\n\n## Variance inflation factors\n\n- VIFs are a measure of the amount of collinearity in the model.\n- Sometimes easier to interpret than pairwise correlation coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(loyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                ABUND         AREA      YR.ISOL       DIST       LDIST\nABUND      1.00000000  0.255970206  0.503357741  0.2361125  0.08715258\nAREA       0.25597021  1.000000000 -0.001494192  0.1083429  0.03458035\nYR.ISOL    0.50335774 -0.001494192  1.000000000  0.1132175 -0.08331686\nDIST       0.23611248  0.108342870  0.113217524  1.0000000  0.31717234\nLDIST      0.08715258  0.034580346 -0.083316857  0.3171723  1.00000000\nGRAZE     -0.68251138 -0.310402417 -0.635567104 -0.2558418 -0.02800944\nALT        0.38583617  0.387753885  0.232715406 -0.1101125 -0.30602220\nAREA_L10   0.74003580  0.584651024  0.278414517  0.3047850  0.33680642\nLDIST_L10  0.11812448  0.101607829 -0.161116108  0.4968169  0.82059568\nDIST_L10   0.12672333  0.163054319 -0.019572228  0.8233190  0.29365797\n                GRAZE        ALT   AREA_L10   LDIST_L10    DIST_L10\nABUND     -0.68251138  0.3858362  0.7400358  0.11812448  0.12672333\nAREA      -0.31040242  0.3877539  0.5846510  0.10160783  0.16305432\nYR.ISOL   -0.63556710  0.2327154  0.2784145 -0.16111611 -0.01957223\nDIST      -0.25584182 -0.1101125  0.3047850  0.49681692  0.82331904\nLDIST     -0.02800944 -0.3060222  0.3368064  0.82059568  0.29365797\nGRAZE      1.00000000 -0.4071671 -0.5590886 -0.03399082 -0.14263922\nALT       -0.40716705  1.0000000  0.2751428 -0.27404380 -0.21900701\nAREA_L10  -0.55908864  0.2751428  1.0000000  0.38247952  0.30216662\nLDIST_L10 -0.03399082 -0.2740438  0.3824795  1.00000000  0.60386637\nDIST_L10  -0.14263922 -0.2190070  0.3021666  0.60386637  1.00000000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(loyn_logfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  YR.ISOL     GRAZE       ALT  AREA_L10 LDIST_L10  DIST_L10 \n 1.804769  2.524814  1.467937  1.911514  2.009749  1.654553 \n```\n:::\n:::\n\n\n- $1$ = no correlation with other predictors\n- $>10$ is a sign for high, not tolerable correlation of model predictors which need to be removed and the model refitted\n\n\n## Variance inflation factors using `performance`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif <- performance::check_collinearity(loyn_logfit)\nvif\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Check for Multicollinearity\n\nLow Correlation\n\n      Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n   YR.ISOL 1.80 [1.40, 2.64]         1.34      0.55     [0.38, 0.72]\n     GRAZE 2.52 [1.85, 3.73]         1.59      0.40     [0.27, 0.54]\n       ALT 1.47 [1.19, 2.16]         1.21      0.68     [0.46, 0.84]\n  AREA_L10 1.91 [1.46, 2.79]         1.38      0.52     [0.36, 0.68]\n LDIST_L10 2.01 [1.52, 2.94]         1.42      0.50     [0.34, 0.66]\n  DIST_L10 1.65 [1.30, 2.42]         1.29      0.60     [0.41, 0.77]\n```\n:::\n:::\n\n\n:::{.callout-note}\nTolerance is the reciprocal of VIF\n:::\n\n## Visualising VIF\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(performance::check_collinearity(loyn_logfit))\n```\n\n::: {.cell-output-display}\n![](Lecture-08a_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n## The best model?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(broom)\nfull6 <- loyn_logfit\npart5 <- update(full6, . ~ . - LDIST_L10)\npart4 <- update(part5, . ~ . - DIST_L10)\npart3 <- update(part4, . ~ . - ALT)\npart2 <- update(part3, . ~ . - YR.ISOL)\npart1 <- update(part2, . ~ . - GRAZE)\n\nformulas <- c(part1$call$formula, \n              part2$call$formula, \n              part3$call$formula, \n              part4$call$formula, \n              part5$call$formula, \n              loyn_logfit$call$formula)\nformulas <-\n  c(\"ABUND ~ AREA_L10\",\n    \"ABUND ~ AREA_L10 + GRAZE\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10\",\n    \"ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10 + LDIST_L10\")\n\nrs <- bind_rows(glance(part1), \n          glance(part2), \n          glance(part3), \n          glance(part4),\n          glance(part5), \n          glance(full6)) %>%\n        mutate(Model = formulas) %>%\n        select(Model, r.squared, adj.r.squared)\n\nknitr::kable(rs)\n```\n\n::: {.cell-output-display}\n|Model                                                           | r.squared| adj.r.squared|\n|:---------------------------------------------------------------|---------:|-------------:|\n|ABUND ~ AREA_L10                                                | 0.5476530|     0.5392762|\n|ABUND ~ AREA_L10 + GRAZE                                        | 0.6527344|     0.6396300|\n|ABUND ~ AREA_L10 + GRAZE + YR.ISOL                              | 0.6732151|     0.6543621|\n|ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT                        | 0.6823418|     0.6574275|\n|ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10             | 0.6843360|     0.6527696|\n|ABUND ~ AREA_L10 + GRAZE + YR.ISOL + ALT + DIST_L10 + LDIST_L10 | 0.6849359|     0.6463567|\n:::\n:::\n\n\n- R-squared increases with addition of predictors.\n- Adj. r-squared *varies* with addition of predictors.\n\n\n## The problem\n\n- Other combinations of predictors exist but are not shown.\n- Need *automated way* to select the best model -- 6 predictors gives us 6! = **720 models** to choose from!\n- Two options:\n  - Backward elimination\n  - Forward selection (not covered in this course)\n\n:::{.callout-note}\nWe focus on backward elimination here, but forward selection is also a viable option that is **not** covered in this course.\n:::\n\n# Backward elimination\n\n## Steps for backward elimination\n\n1. Start with full model.\n2. For each predictor, test the effect of its removal on the model fit.\n3. Remove the predictor that has the *least* effect on the model fit i.e. the **least informative** predictor, unless it is nonetheless supplying significant information about the response.\n4. Repeat steps 2 and 3 until no predictors can be removed without significantly affecting the model fit.\n\nIn backward selection, the model fit is assessed using the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). **Here we focus on the AIC.**\n\n## In R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nback_step <- step(loyn_logfit, direction = \"backward\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=214.14\nABUND ~ (AREA + YR.ISOL + DIST + LDIST + GRAZE + ALT + AREA_L10 + \n    LDIST_L10 + DIST_L10) - AREA - LDIST - DIST\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1      3.80 2000.7 212.25\n- DIST_L10   1      4.68 2001.5 212.27\n- ALT        1     27.02 2023.9 212.90\n<none>                   1996.8 214.14\n- YR.ISOL    1    108.83 2105.7 215.11\n- GRAZE      1    131.07 2127.9 215.70\n- AREA_L10   1   1059.75 3056.6 235.98\n\nStep:  AIC=212.25\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n- DIST_L10  1     12.64 2013.3 210.60\n- ALT       1     35.12 2035.8 211.22\n<none>                  2000.7 212.25\n- YR.ISOL   1    121.64 2122.3 213.55\n- GRAZE     1    132.44 2133.1 213.84\n- AREA_L10  1   1193.04 3193.7 236.44\n\nStep:  AIC=210.6\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n- ALT       1     57.84 2071.1 210.19\n<none>                  2013.3 210.60\n- GRAZE     1    123.48 2136.8 211.94\n- YR.ISOL   1    134.89 2148.2 212.23\n- AREA_L10  1   1227.11 3240.4 235.25\n\nStep:  AIC=210.19\nABUND ~ YR.ISOL + GRAZE + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n<none>                  2071.1 210.19\n- YR.ISOL   1    129.81 2200.9 211.59\n- GRAZE     1    188.45 2259.6 213.06\n- AREA_L10  1   1262.97 3334.1 234.85\n```\n:::\n:::\n\n\nBefore we interpret... let's look at the new column, **AIC**.\n\n# Akaike Information Criterion (AIC)\n\n## History of AIC\n\n$$AIC = 2k - 2\\ln(L)$$\n\n- Most **popular** model selection criterion.\n- Developed by [Hirotsugu Akaike](https://en.wikipedia.org/wiki/Hirotugu_Akaike) under the name of \"an information criterion\" (AIC)\n- Founded on information theory which is concerned with the transmission, processing, utilization, and extraction of information.\n\n## Formula {auto-animate=\"true\"}\n\n$$AIC = 2k - 2\\ln(L)$$\n\nwhere $k$ is the number of parameters in the model and $L$ is the maximum value of the likelihood function. When used in linear regression, the AIC can also be defined as:\n\n$$AIC = n\\log(\\frac{RSS}{n}) + 2k$$\n\nwhere $RSS$ is the residual sum of squares and $2k$ is the number of parameters in the model (i.e. model complexity).\n\n## Interpretation {auto-animate=\"true\"}\n\n$$AIC = n\\log(\\frac{RSS}{n}) + 2k$$\n\n- The AIC is a measure of the **relative quality** or **goodness of fit** of a statistical model for a given set of data.\n- It estimates the **relative amount of information** lost by a given model when it is used to approximate the true underlying process that generated the data.\n- **The smaller the AIC, the better the model fits the data.**\n- A *relative* measure and *unitless*, so it is not worth trying to interpret alone.\n\n## Back to our example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nback_step <- step(loyn_logfit, direction = \"backward\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=214.14\nABUND ~ (AREA + YR.ISOL + DIST + LDIST + GRAZE + ALT + AREA_L10 + \n    LDIST_L10 + DIST_L10) - AREA - LDIST - DIST\n\n            Df Sum of Sq    RSS    AIC\n- LDIST_L10  1      3.80 2000.7 212.25\n- DIST_L10   1      4.68 2001.5 212.27\n- ALT        1     27.02 2023.9 212.90\n<none>                   1996.8 214.14\n- YR.ISOL    1    108.83 2105.7 215.11\n- GRAZE      1    131.07 2127.9 215.70\n- AREA_L10   1   1059.75 3056.6 235.98\n\nStep:  AIC=212.25\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10 + DIST_L10\n\n           Df Sum of Sq    RSS    AIC\n- DIST_L10  1     12.64 2013.3 210.60\n- ALT       1     35.12 2035.8 211.22\n<none>                  2000.7 212.25\n- YR.ISOL   1    121.64 2122.3 213.55\n- GRAZE     1    132.44 2133.1 213.84\n- AREA_L10  1   1193.04 3193.7 236.44\n\nStep:  AIC=210.6\nABUND ~ YR.ISOL + GRAZE + ALT + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n- ALT       1     57.84 2071.1 210.19\n<none>                  2013.3 210.60\n- GRAZE     1    123.48 2136.8 211.94\n- YR.ISOL   1    134.89 2148.2 212.23\n- AREA_L10  1   1227.11 3240.4 235.25\n\nStep:  AIC=210.19\nABUND ~ YR.ISOL + GRAZE + AREA_L10\n\n           Df Sum of Sq    RSS    AIC\n<none>                  2071.1 210.19\n- YR.ISOL   1    129.81 2200.9 211.59\n- GRAZE     1    188.45 2259.6 213.06\n- AREA_L10  1   1262.97 3334.1 234.85\n```\n:::\n:::\n\nPrinting `back_step` reveals the final model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nback_step\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + GRAZE + AREA_L10, data = loyn)\n\nCoefficients:\n(Intercept)      YR.ISOL        GRAZE     AREA_L10  \n -134.26065      0.07835     -1.90216      7.16617  \n```\n:::\n:::\n\n\n## Backward elimination: coefficients\n\n**Full model**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsjPlot::tab_model(\n  loyn_logfit, back_step, \n  show.ci = FALSE, \n  show.aic = TRUE,\n  dv.labels = c(\"Full model\",\n                \"Backward model\")\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Full model</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Backward model</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;125.70</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.177</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;134.26</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.126</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">YR ISOL</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.07</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.109</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.08</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.077</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">GRAZE</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.67</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.079</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.90</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>0.034</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">ALT</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.02</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.419</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">AREA L10</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">7.47</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">7.17</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">LDIST L10</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.65</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.761</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">DIST L10</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.91</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.736</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">56</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">56</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.685 / 0.646</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.673 / 0.654</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">AIC</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">375.064</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">371.109</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\nThe backward model retains more explanatory power than the full model!\n\n# Summary\n\n## Model selection\n\n**Model development**\n\n1. Start with full model and check assumptions (e.g. normality, homoscedasticity, linearity, etc.).\n2. Look for additional issues (e.g. multicollinearity, outliers, etc.) - look at correlations, leverage, VIF plots.\n3. Consider transformations (e.g. log, sqrt, etc.).\n4. Test assumptions again.\n\n**Model selection**\n\n5. Use VIF as an initial step to get rid of highly correlated predictors.\n6. Perform variable selection using backward elimination (good and fast), because:\n   - Using r~2~ as a criterion is *not* recommended (it is not a good measure of model fit, only a good measure of variance explained). \n   - Using adjuster r~2~ is better, but has the same issue as r~2~.\n   - Using partial F-test is good, but slow.\n\n# Next lecture\n\n## Next lecture: model interpretation and prediction\n\n- How to incorporate cross-validation into your workflow\n- Determining prediction intervals and performance metrics\n\n\n\n<!-- \n## Abalone\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load abalone data\nlibrary(AppliedPredictiveModeling)\ndata(abalone)\nglimpse(abalone)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 4,177\nColumns: 9\n$ Type          <fct> M, M, F, M, I, I, F, F, M, F, F, M, M, F, F, M, I, F, M,…\n$ LongestShell  <dbl> 0.455, 0.350, 0.530, 0.440, 0.330, 0.425, 0.530, 0.545, …\n$ Diameter      <dbl> 0.365, 0.265, 0.420, 0.365, 0.255, 0.300, 0.415, 0.425, …\n$ Height        <dbl> 0.095, 0.090, 0.135, 0.125, 0.080, 0.095, 0.150, 0.125, …\n$ WholeWeight   <dbl> 0.5140, 0.2255, 0.6770, 0.5160, 0.2050, 0.3515, 0.7775, …\n$ ShuckedWeight <dbl> 0.2245, 0.0995, 0.2565, 0.2155, 0.0895, 0.1410, 0.2370, …\n$ VisceraWeight <dbl> 0.1010, 0.0485, 0.1415, 0.1140, 0.0395, 0.0775, 0.1415, …\n$ ShellWeight   <dbl> 0.150, 0.070, 0.210, 0.155, 0.055, 0.120, 0.330, 0.260, …\n$ Rings         <int> 15, 7, 9, 10, 7, 8, 20, 16, 9, 19, 14, 10, 11, 10, 10, 1…\n```\n:::\n:::\n\n\nCan we predict the age of abalone from physical measurements? -->\n\n\n\n# Thanks!\n\n**Questions? Comments?**\n\nSlides made with [Quarto](https://quarto.org)\n",
    "supporting": [
      "Lecture-08a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}